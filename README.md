# Disaster-Tweets-Kaggle
My attempt on the Disaster Tweet project on Kaggle. 

Note: This code will be updated over time.

## Models Used in this Project:
    1. XGBoost
    
    Models that I would like to try: 
    • Logistic Regression, 
    • Neural Network, 
    • Decision Trees, 
    • k-Nearest Neighbors
    
## Findings from this Project
  When it comes to Natural Language Processing (NLP), the cleaning of the data is the tough part. 
  As of 2020, i've only had the chance to work on 2 NLP projects and have used the same methods to clean.
  Obviously, i would need to have a lot more practice with cleaning strings through regex, stop words, lemmatization and stemming. 
  I would also like to learn more tips and tricks that top-tier data scientists use for processing language data on top of those that i've mentioned.
